# =============================================================================
# MCP Client Configuration
# =============================================================================

# AI Provider Selection
# Choose one: "openai", "claude", or "ollama"
AI_PROVIDER=openai

# =============================================================================
# OpenAI Configuration (required if AI_PROVIDER=openai)
# =============================================================================
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini

# Available OpenAI Models:
# - gpt-4o                  (Latest GPT-4 Omni model)
# - gpt-4o-mini             (Faster, cheaper GPT-4 Omni)
# - gpt-4-turbo             (GPT-4 Turbo)
# - gpt-4                   (GPT-4 base model)
# - gpt-3.5-turbo           (ChatGPT model)
# - gpt-3.5-turbo-16k       (ChatGPT with 16k context)

# =============================================================================
# Anthropic Claude Configuration (required if AI_PROVIDER=claude)
# =============================================================================
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
# CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Available Claude Models:
# - claude-3-5-sonnet-20241022  (Latest Claude 3.5 Sonnet)
# - claude-3-opus-20240229      (Most capable Claude 3)
# - claude-3-sonnet-20240229    (Balanced Claude 3)
# - claude-3-haiku-20240307     (Fastest Claude 3)

# =============================================================================
# Ollama Configuration (required if AI_PROVIDER=ollama)
# =============================================================================
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2

# Available Ollama Models (examples - depends on what you have installed):
# - llama3.2                (Latest Llama 3.2)
# - llama3.1                (Llama 3.1)
# - qwen2.5                 (Qwen 2.5)
# - mistral                 (Mistral 7B)
# - codellama               (Code Llama)
# - gemma2                  (Google Gemma 2)
# - phi3                    (Microsoft Phi-3)
# - deepseek-coder          (DeepSeek Coder)
# - starcoder2              (StarCoder 2)
# - yi                      (Yi models)
# - solar                   (Solar models)

# =============================================================================
# Optional Configuration
# =============================================================================

# Debug Mode
# Set to 'true' to enable detailed debug logging
# DEBUG=true

# Node.js Options
# NODE_ENV=development
# NODE_OPTIONS=--max-old-space-size=4096

# =============================================================================
# MCP Server Examples
# =============================================================================

# When using the client, you can connect to various MCP servers:
#
# Playwright MCP (Browser automation):
# npx @playwright/mcp@latest --port 8931 --isolated
# Usage: node build/index.js http://localhost:8931/sse
#
# File system MCP server:
# Usage: node build/index.js ./path/to/filesystem-server.py
#
# Custom MCP server:
# Usage: node build/index.js ./path/to/your-server.js

# =============================================================================
# Example Complete Configurations
# =============================================================================

# Example 1: OpenAI with GPT-4o-mini
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-...
# OPENAI_MODEL=gpt-4o-mini

# Example 2: Claude with Sonnet
# AI_PROVIDER=claude
# ANTHROPIC_API_KEY=sk-ant-...
# CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Example 3: Local Ollama with Llama
# AI_PROVIDER=ollama
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2

# Example 4: Remote Ollama instance
# AI_PROVIDER=ollama
# OLLAMA_URL=http://192.168.1.100:11434
# OLLAMA_MODEL=qwen2.5 